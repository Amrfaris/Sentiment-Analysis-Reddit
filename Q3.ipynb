{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\a_lolooh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "\n",
    "#### this class is purely for organisation and ease of access, instead of messy nested array access , we just create a posts object with attributes ( the post itself, its emotion, its sentiment )\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize as tokenize\n",
    "class Posts : \n",
    "    def __init__(self, post, emotion, sentiment):\n",
    "        self.post = post\n",
    "        self.emotion = emotion\n",
    "        self.sentiment = sentiment\n",
    "        self.tokens = list(tokenize(post))\n",
    "        \n",
    "    def print(self):\n",
    "        print(f\"post: {self.post} emotion: {self.emotion} sentiment: {self.sentiment}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "\n",
    "posts = []\n",
    "\n",
    "# the file given to us is a gzip which has a json inside it, so we need to unzip first and then load the json file \n",
    "\n",
    "with gzip.open('goemotions.json.gz', 'r') as f: # unzipping\n",
    "    data = json.loads(f.read(), encoding=\"utf-8\") # loading json\n",
    "    for line in data:\n",
    "        posts.append(Posts(line[0],line[1],line[2])) #creating the object and appending to the list \n",
    "        \n",
    "# basically posts is a list of objects where each object has its info as attributes ( see above )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3 Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\a_lolooh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import nltk \n",
    "import joblib\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Downloading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - 3.3 Tokenization and Embeddings\n",
    "#### Tokens were implicitly generated during object instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "hit = 0\n",
    "miss = 0 \n",
    "\n",
    "for i, post in enumerate(posts):\n",
    "    A = [] \n",
    "    ll = post.tokens\n",
    "    for token in ll:\n",
    "        hit += 1\n",
    "        try:\n",
    "            A.append(model[token])\n",
    "        except KeyError:\n",
    "            miss+=1\n",
    "            continue\n",
    "    if not A: # if no word2vec fill in zeroes \n",
    "        post.embedding = np.zeros(300, dtype='int')\n",
    "        continue\n",
    "    np_arr = np.vstack(A)\n",
    "    avg = np.average(np_arr, axis=0)\n",
    "    post.embedding = np.copy(avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens is : 2642139\n"
     ]
    }
   ],
   "source": [
    "print (f\"Number of tokens is : {hit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x = np.array([x.embedding for x in posts])\n",
    "y_emotion = np.array([x.emotion for x in posts])\n",
    "y_sentiment = np.array([x.sentiment for x in posts])\n",
    "\n",
    "trainX, testX, trainY_emotion,  testY_emotion = train_test_split(x, y_emotion, test_size=0.2, random_state = 42) \n",
    "trainX, testX, trainY_sent,  testY_sent = train_test_split(x, y_sentiment, test_size=0.2, random_state = 42) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Computing Hit Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainxHit, testxHit, trainYHit,  testYHit = train_test_split(np.array([x.post for x in posts]), y_emotion, test_size=0.2, random_state = 42) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_train = 0\n",
    "miss_train = 0 \n",
    "\n",
    "for post in trainXHit:\n",
    "    for token in tokenize(post):\n",
    "        dummyA = []\n",
    "        hit_train += 1\n",
    "        try:\n",
    "            dummyA.append(model[token])\n",
    "        except KeyError:\n",
    "            miss_train+=1\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_test = 0\n",
    "miss_test = 0 \n",
    "\n",
    "for post in testxHit:\n",
    "    for token in tokenize(post):\n",
    "        dummyA = []\n",
    "        hit_test += 1\n",
    "        try:\n",
    "            dummyA.append(model[token])\n",
    "        except KeyError:\n",
    "            miss_test+=1\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit rate for Training Set: 1.0 \n",
      "Hit rate for Test Set: 0.7741150983877424 \n",
      "Overall Hit rate: 0.7745073215300179 \n"
     ]
    }
   ],
   "source": [
    "print(f\"Hit rate for Training Set: {((hit_train-miss_train)/hit_train)} \")\n",
    "print(f\"Hit rate for Test Set: {((hit_test-miss_test)/hit_test)} \")\n",
    "print(f\"Overall Hit rate: {((hit-miss)/hit)} \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 BASE MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "mlp_emotion = MLPClassifier(random_state=2, max_iter=20).fit(trainX, trainY_emotion)\n",
    "y_pred_base_emotion = mlp_emotion.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41851938074729367"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_emotion_score = mlp_emotion.score(testX,testY_emotion)\n",
    "mlp_emotion_score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_sent = MLPClassifier(random_state=2, max_iter=20).fit(trainX, trainY_sent)\n",
    "y_pred_base_sent = mlp_sent.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5504306832731929"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_sent_score = mlp_sent.score(testX,testY_sent)\n",
    "mlp_sent_score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 TOP MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=MLPClassifier(max_iter=2), n_jobs=-1,\n",
       "             param_grid={'activation': ['relu', 'tanh', 'identity'],\n",
       "                         'hidden_layer_sizes': [(10, 5), (15, 10)],\n",
       "                         'solver': ['adam', 'sgd']},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_gs = MLPClassifier(max_iter=2)\n",
    "parameter_space = {\n",
    "    'solver': [\"adam\", \"sgd\"],\n",
    "    'hidden_layer_sizes' : [(10,5),(15,10)],\n",
    "    'activation' : [\"relu\", \"tanh\", \"identity\"]\n",
    "}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf_emotion = GridSearchCV(mlp_gs, parameter_space, n_jobs=-1, cv=5, verbose = 2)\n",
    "clf_emotion.fit(trainX, trainY_emotion) # X is train samples and y is the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'activation': 'tanh', 'hidden_layer_sizes': (15, 10), 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters found:\\n', clf_emotion.best_params_)\n",
    "y_pred_top_emotion = clf_emotion.predict(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    }
   ],
   "source": [
    "clf_sent = GridSearchCV(mlp_gs, parameter_space, n_jobs=-1, cv=5, verbose = 2)\n",
    "top_sent = clf_sent.fit(trainX, trainY_sent) # X is train samples and y is the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'activation': 'relu', 'hidden_layer_sizes': (15, 10), 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters found:\\n', clf_sent.best_params_)\n",
    "y_pred_top_sent = clf_sent.predict(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix , ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(conf,clas, file, title): # expects conf. matrix, classification report, file path and title as \n",
    "    with open(file, 'a') as f: # \"a\" appends text to file\n",
    "        f.write(f\"{title}\\n\\nConfusion Matrix: \\n{conf}\\n\\nCLassification Report: \\n{clas}\\n\\n\\n\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BASE MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "base_matrix_emotion = confusion_matrix(testY_emotion,y_pred_base_emotion)\n",
    "base_clas_emotion = classification_report(testY_emotion, y_pred_base_emotion)\n",
    "\n",
    "write_to_file(base_matrix_emotion,base_clas_emotion, 'performance.txt','WORD2VEC BASE MLP - EMOTION')\n",
    "\n",
    "base_matrix_sent = confusion_matrix(testY_sent,y_pred_base_sent)\n",
    "base_clas_sent = classification_report(testY_sent, y_pred_base_sent)\n",
    "\n",
    "write_to_file(base_matrix_sent,base_clas_sent, 'performance.txt','WORD2VEC BASE MLP - SENTIMENT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TOP MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_matrix_emotion = confusion_matrix(testY_emotion,y_pred_top_emotion)\n",
    "top_clas_emotion = classification_report(testY_emotion, y_pred_top_emotion)\n",
    "\n",
    "write_to_file(top_matrix_emotion,top_clas_emotion, 'performance.txt','WORD2VEC TOP MLP - EMOTION')\n",
    "\n",
    "top_matrix_sent = confusion_matrix(testY_sent,y_pred_top_sent)\n",
    "top_clas_sent = classification_report(testY_sent, y_pred_top_sent)\n",
    "\n",
    "write_to_file(base_matrix_sent,base_clas_sent, 'performance.txt','WORD2VEC TOP MLP - SENTIMENT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit2 = 0\n",
    "miss2 = 0 \n",
    "\n",
    "for i, post in enumerate(posts):\n",
    "    A = []\n",
    "    ll = post.tokens\n",
    "    for token in ll:\n",
    "        hit2 += 1\n",
    "        try:\n",
    "            A.append(model2[token])\n",
    "        except KeyError:\n",
    "            miss2+=1\n",
    "            continue\n",
    "    if not A: # if no word2vec fill in zeroes \n",
    "        post.embedding2 = np.zeros(50, dtype='int')\n",
    "        continue\n",
    "    np_arr = np.vstack(A)\n",
    "    avg = np.average(np_arr, axis=0)\n",
    "    post.embedding2 = np.copy(avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = np.array([x.embedding2 for x in posts])\n",
    "\n",
    "\n",
    "trainX2, testX2, trainY2_emotion,  testY2_emotion = train_test_split(x2, y_emotion, test_size=0.2, random_state = 42) \n",
    "trainX2, testX2, trainY2_sent,  testY2_sent = train_test_split(x2, y_sentiment, test_size=0.2, random_state = 42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mlp_emotion2 = MLPClassifier(random_state=2, max_iter=20)\n",
    "mlp_emotion2.fit(trainX2, trainY2_emotion)\n",
    "y_pred_base_emotion2 = mlp_emotion2.predict(testX2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35167617273891283"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_base_emotion2 = mlp_emotion2.predict(testX2)\n",
    "mlp_emotion_score2 = mlp_emotion2.score(testX2,testY2_emotion)\n",
    "mlp_emotion_score2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_sent2 = MLPClassifier(random_state=2, max_iter=20).fit(trainX2, trainY2_sent)\n",
    "\n",
    "\n",
    "y_pred_base_sent2 = mlp_sent2.predict(testX2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46592364101967176"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_sent_score2 = mlp_sent2.score(testX2,testY2_sent)\n",
    "mlp_sent_score2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_matrix_emotion2 = confusion_matrix(testY2_emotion,y_pred_base_emotion2)\n",
    "base_clas_emotion2 = classification_report(testY_emotion, y_pred_base_emotion2)\n",
    "\n",
    "write_to_file(base_matrix_emotion2,base_clas_emotion2, 'performance.txt','WORD2VEC MODEL 2 BASE MLP - EMOTION')\n",
    "\n",
    "base_matrix_sent2 = confusion_matrix(testY2_sent,y_pred_base_sent2)\n",
    "base_clas_sent2 = classification_report(testY2_sent, y_pred_base_sent2)\n",
    "\n",
    "write_to_file(base_matrix_sent2,base_clas_sent2, 'performance.txt','WORD2VEC MODEL 2 BASE MLP - SENTIMENT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = api.load(\"fasttext-wiki-news-subwords-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit3 = 0\n",
    "miss3 = 0 \n",
    "\n",
    "for i, post in enumerate(posts):\n",
    "    A = []\n",
    "    ll = post.tokens\n",
    "    for token in ll:\n",
    "        hit3 += 1\n",
    "        try:\n",
    "            A.append(model3[token])\n",
    "        except KeyError:\n",
    "            miss3+=1\n",
    "            continue\n",
    "    if not A: # if no word2vec fill in zeroes \n",
    "        post.embedding3 = np.zeros(300, dtype='int')\n",
    "        continue\n",
    "    np_arr = np.vstack(A)\n",
    "    avg = np.average(np_arr, axis=0)\n",
    "    post.embedding3 = np.copy(avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = np.array([x.embedding3 for x in posts])\n",
    "\n",
    "\n",
    "trainX3, testX3, trainY3_emotion,  testY3_emotion = train_test_split(x3, y_emotion, test_size=0.2, random_state = 42) \n",
    "trainX3, testX3, trainY3_sent,  testY3_sent = train_test_split(x3, y_sentiment, test_size=0.2, random_state = 42) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mlp_emotion3 = MLPClassifier(random_state=2, max_iter=20)\n",
    "mlp_emotion3.fit(trainX3, trainY3_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_base_emotion3 = mlp_emotion3.predict(testX3)\n",
    "mlp_emotion_score3 = mlp_emotion3.score(testX3,testY3_emotion)\n",
    "mlp_emotion_score3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mlp_sent3 = MLPClassifier(random_state=2, max_iter=20)\n",
    "mlp_sent3.fit(trainX3, trainY3_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_base_sent3 = mlp_sent3.predict(testX3)\n",
    "mlp_sent_score3 = mlp_sent3.score(testX3,testY3_sent)\n",
    "mlp_sent_score3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_matrix_emotion3 = confusion_matrix(testY3_emotion,y_pred_base_emotion3)\n",
    "base_clas_emotion3 = classification_report(testY_emotion, y_pred_base_emotion3)\n",
    "\n",
    "write_to_file(base_matrix_emotion3,base_clas_emotion3, 'performance.txt','WORD2VEC MODEL 3 BASE MLP - EMOTION')\n",
    "\n",
    "base_matrix_sent3 = confusion_matrix(testY3_sent,y_pred_base_sent3)\n",
    "base_clas_sent3 = classification_report(testY3_sent, y_pred_base_sent3)\n",
    "\n",
    "write_to_file(base_matrix_sent3,base_clas_sent3, 'performance.txt','WORD2VEC MODEL 3 BASE MLP - SENTIMENT')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
